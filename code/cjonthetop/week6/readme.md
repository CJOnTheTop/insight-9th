# 회귀 분석

< 세 줄 요약 >
- 회귀는 입력을 받아 적합한 숫자를 예측하는 문제
- 단순선형회귀와 다중선형회귀의 차이는 종속변수에 독립변수가 1개/n개이냐의 차이이다.



## 이론 설명
- 문제 유형은 크게 3가지가 있다. (회귀 vs 분류 vs 군집화)

### 회귀
- 입력을 받아 적합한 숫자를 예측하는 문제
- 가장 기본적인 머신러닝
- 회귀 문제 해결 기법 _ 선형 회귀, 가우시안 프로세스 회귀, 칼만 필터

### 분류
- 분류에 대한 손실함수를 직접 최적화하여 푸는 방법
- 주어진 입력에 대하여 여러가지 항목들 중 제일 가장 높은 가능성 한가지
- ex) 어떤 이미지가 어떤 항목에 속하는 가, 생산 공정에서 데이터에 하자가 있는가, 어떤 의미가 있는가


### 군집화
- 비슷한 성격의 데이터를 묶는 문제



## 단순선형 회귀 (simple linear regression)
- 기계학습은 본질적으로 'probabliy approximately correct'한 함수를 학습하는 과정이다. 선형 회귀(linear regression)은 해당 target 함수를 선형의 함수로서 근사하는 방법
- target 함수를 각 feature variable 들의 선형 가중합이라 생각할 수 있음

### 산점도(scatter plot)그리기
- 각 data point들을 2차원 산점도로 나타내어 linearity를 가지는 지 시각화하여 눈으로 확인

### 회귀계수 추정
- 회귀 계수를 추정하는 방법은 크게 최소제곱법(최소자승법)과 최대우도추정법 2가지가 있음
- MAE(오차율)을 줄이는 것이 핵심


### 유의성 검증
- 유의서 검증 == 모델에서 사용하는 독립변수들이 종속 변수에 대한 유의미한 영향을 미치는 것인지 검증
- 모형 유의성은 분산 분석과 같이 각각을 자유도로 나눈 평균 변동을 기준으로 평가한다.


### 회귀 모형의 평가
- 평균제곱근 오차(root mean square error)RSME는 추정 모형의 예측 정확도 평가를 위한 지표이다.
- RSME는 회귀선을 기준으로 실측치(ground truth)가 얼마나 벗어나 있는지 나타내는 값이다. (작을수록 잘 회귀선이 fit 되어있다)





## 다중선형 회귀 (multiple linear regression)
- 단순 선형 회귀모형의 확장으로 연속형 반응(종속) 변수 하나에 설명(독립)변수가 둘 이상인 모형을 의미한다.
- 당연히 설명 변수가 늘어나기 때문에 추가로 검토해야할 문제가 생긴다.
- 대표적으로 설명변수의 정보 중첩이 발생하는 다중공선성(multicollinearity)문제와 너무 많은 설명 변수를 포함해 발생하는 차원의 저주 문제가 있다.

- 특성(attribute)마다 가중치를 다르게 할건데. 역시 여기서도 "가중치"를 구하는 문제가 된다.



## 회귀 모형 가정 진단
- 회귀모형은 반응 변수와 설명 변수의 선형 관계를 전제로 하기 때문에, 벌써 bias가 들어가있다.
- 또한 오차에 대한 독립성, 정규성, 등분산성 가정을 전제로 한다.


### 선형성 진단
- 가장 기초가 되는 진단, -> 반응변수와 설명변수가 선형관계임은 기본으로 하기 때문

### 오차항의 독립성 진단
- 오차의 독립성 (independence) 문제는 고나측치가 서로 상관되어 있을 때, 발생할 수 있다.
- 시간에 따른 관측된 자료의 경우 시점 데이터가 시점 값에 영향을 주기도 한다.
    - 잔차 그래프
    - ACF 그래프
    - Durbin-Watson 통계랑
    
- 표준화 잔차가 랜덤하게 분포하지 않고 경향성을 가지는 경우, 잔차의 독립성을 의심해야 한다.
- ACF(Auto-Correlation Function)을 이용해서 독립성 만족 여부를 확인할 수 있다. ACF그래프에서 0시차 이후에 신뢰구간을 벗어나는 경우 독립성을 의심해봐야 함.

### 오차항의 등분산성 진단
- 잔차 그래프와 등분산성 검정을 통해 알 수 있다


### 오차항의 정규성 진단
- 오차항의 정규성(normality)은 잔차에 대한 Q-Q plot을 이용한 시각적 확인 방법과 정규성 검정을 통한 방법이 있다.



## 회귀 모형 평가
- linear regression한 모델은 어느 정도의 오차를 가지는가?

### 평균 제곱 오차
- MSE(mean squared error) 또는 MSD 라고도 하며 다음과 같이 표현된다.
- 모형을 통해 추정된 값(predict)과 실측값(ground truth)의 차에 대한 제곱합의 평균으로 표현되며, 이 값이 떨어지면 예측값이 떨어진다.
- 평균제곱근의 오차 / 결정계수 / 수정계수 / akaike 정보 기준 / schwarz 베이지안 기준 / mape 등


## 선형 회귀 분석의 주요 이슈
- 언제까지나 가정이 존재하고, 오차(error)를 다루기 때문에 다양한 이슈가 있기 마련이다.
    - 범주형 설명 변수가 존재하는 경우
    - 데이터에 이상치가 존재하는 경우
    - 독립 변수 간 선형 상관관계가 존재하는 경우
    
### 이상치
- 이상치(outlier)는 관찰된 데이터의 속성과 많이 떨어진 데이터를 의미
- 어떻게 처리해야 하는지 각각에 case에 따라 다르기도 하고, 어떤 method를 사용하여 학습할건지에 따라서 달라지 


### 다중공선성
- 설명 변수들 간의 선형 종속(linear dependency)이 심한 경우를 말합니다.
- 즉, 설명 변수들 간 정보의 중첩이 발생한 것이다. (main이 되는 feature를 아직 뽑지 못했다. 혹은 전처리(pca)가 완료 되지 않음)

- 완전한 선형 종속관계가 발생한 경우, 다중공선성(perfect multicollinearity)를 갖는다.
    - 1. 변수 선택
    - 2. 변수 변환
    - 3. regularization
    
    


